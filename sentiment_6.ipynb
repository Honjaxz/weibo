{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0a06bdc-d17c-4a3e-bc80-802a548f3b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from collections import defaultdict\n",
    "from textwrap import wrap\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e06889cb-5588-4726-97a3-9d3fe2d6edb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'autodl-tmp/ernie-1.0-base-zh' #模型的位置\n",
    "PATH = 'autodl-tmp/bilstm_model.pth'  # 定义模型保存路径\n",
    "TRAIN_DATA_PATH = \"autodl-tmp/train_data/all_train.csv\"\n",
    "TEST_DATA_PATH = \"autodl-tmp/train_data/all_test.csv\"\n",
    "LABEL_DICT = {'happy':0, 'sad':1, 'neutral':2, 'fear':3, 'angry':4, 'surprise':5}# 标签映射表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f91ca86-a6d6-4a81-bec5-b47b7d91b4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据进行token化处理, seq_length表示接受的句子最大长度\n",
    "def convert_text_to_token(tokenizer, sentence, seq_length):\n",
    "    tokens = tokenizer.tokenize(sentence) # 句子转换成token\n",
    "    tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"] # token前后分别加上[CLS]和[SEP]\n",
    "    # 生成 input_id, seg_id, att_mask\n",
    "    ids1 = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    types = [0] * len(ids1)\n",
    "    masks = [1] * len(ids1)\n",
    "    # 句子长度统一化处理：截断或补全至seq_length\n",
    "    if len(ids1) < seq_length: #补全\n",
    "        ids = ids1 + [0] * (seq_length - len(ids1)) #[0]是因为词表中PAD的索引是0\n",
    "        types = types + [1] * (seq_length - len(ids1))  # [1]表明该部分为PAD\n",
    "        masks = masks + [0] * (seq_length - len(ids1)) # PAD部分，attention mask置为[0]\n",
    "    else: # 截断\n",
    "        ids = ids1[:seq_length]\n",
    "        types = types[:seq_length]\n",
    "        masks = masks[:seq_length]\n",
    "    assert len(ids) == len(types) == len(masks)\n",
    "    return ids, types, masks\n",
    "\n",
    "# 构造训练集和测试集的DataLoader\n",
    "def genDataLoader(is_train):\n",
    "    if is_train: # 构造训练集\n",
    "        path = TRAIN_DATA_PATH\n",
    "    else: # 构造测试集\n",
    "        path = TEST_DATA_PATH\n",
    "    ### json\n",
    "#     with open(path, encoding='utf8') as f:\n",
    "#         data = json.load(f)\n",
    "    ### csv\n",
    "    data = pd.read_csv(path)\n",
    "    # data.dropna(axis=0,subset = [\"cleaned_content_without_seg\"])   # 丢弃‘cleaned_content_without_seg’这两列中有缺失值的行  \n",
    "    \n",
    "    ids_pool = []\n",
    "    types_pool = []\n",
    "    masks_pool = []\n",
    "    target_pool = []\n",
    "    count = 0\n",
    "    # 遍历构造每条数据\n",
    "    for each1, each2 in zip(data['文本'], data['情绪标签']): # json: for each in data:\n",
    "        cur_ids, cur_type, cur_mask = convert_text_to_token(TOKENIZER, str(each1), seq_length = SEQ_LENGTH) # json: each['cleaned_content']\n",
    "        ids_pool.append(cur_ids)\n",
    "        types_pool.append(cur_type)\n",
    "        masks_pool.append(cur_mask)\n",
    "        cur_target = LABEL_DICT[each2] # json: each['label']\n",
    "        target_pool.append([cur_target])\n",
    "        count += 1\n",
    "        if count % 1000 == 0:\n",
    "            print('已处理{}条'.format(count))\n",
    "            # break\n",
    "    # 构造loader\n",
    "    data_gen = TensorDataset(torch.LongTensor(np.array(ids_pool)),\n",
    "                  torch.LongTensor(np.array(types_pool)),\n",
    "                  torch.LongTensor(np.array(masks_pool)),\n",
    "                  torch.LongTensor(np.array(target_pool)))\n",
    "    # print('shit')\n",
    "    sampler = RandomSampler(data_gen)\n",
    "    loader = DataLoader(data_gen, sampler=sampler, batch_size=BATCH_SIZE)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76304dad-ab4a-4690-9c79-a6aa62aaee6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "# from build_data import genDataLoader\n",
    "from transformers import BertModel\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm # 注意不要直接 import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 复用模型结构\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Model, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(MODEL_NAME)  # /roberta-wwm-ext pretrain/\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True  # 所有参数求梯度\n",
    "        self.fc = nn.Linear(768, num_classes)   # 768 -> 6\n",
    "    def forward(self, x, token_type_ids, attention_mask):\n",
    "        context = x  # 输入的句子\n",
    "        types = token_type_ids\n",
    "        mask = attention_mask  # 对padding部分进行mask，和句子相同size，padding部分用0表示，如：[1, 1, 1, 1, 0, 0]\n",
    "        _, pooled = self.bert(context, token_type_ids=types, attention_mask=mask)\n",
    "        out = self.fc(pooled)   # 得到6分类概率\n",
    "        return out\n",
    "\n",
    "\n",
    "class BERT_BiLSTM(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_classes, n_layers, bidirectional=True, drop_prob=0.5):\n",
    "        super(BERT_BiLSTM, self).__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        # Bert ----------------重点，bert模型需要嵌入到自定义模型里面\n",
    "        self.bert = BertModel.from_pretrained(MODEL_NAME)\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True  # 所有参数求梯度\n",
    "            \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(768, hidden_dim, n_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        # linear and sigmoid layers\n",
    "        if bidirectional:\n",
    "            self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x, token_type_ids, attention_mask):\n",
    "        batch_size = x.size(0)\n",
    "        # 生成bert字向量\n",
    "        x = self.bert(x)[0]  # bert 字向量\n",
    "#         print(\"&&&&&&&&&&&&&&&&\")\n",
    "#         print(x.shape)\n",
    "\n",
    "        # lstm_out\n",
    "        # x = x.float()\n",
    "        lstm_out, (hidden_last, cn_last) = self.lstm(x)\n",
    "        # print(lstm_out.shape)   #[32,100,768]\n",
    "        # print(hidden_last.shape)   #[4, 32, 384]\n",
    "        # print(cn_last.shape)    #[4, 32, 384]\n",
    "\n",
    "        # 修改 双向的需要单独处理\n",
    "        if self.bidirectional:\n",
    "            # 正向最后一层，最后一个时刻\n",
    "            hidden_last_L = hidden_last[-2]\n",
    "            # print(hidden_last_L.shape)  #[32, 384]\n",
    "            # 反向最后一层，最后一个时刻\n",
    "            hidden_last_R = hidden_last[-1]\n",
    "            # print(hidden_last_R.shape)   #[32, 384]\n",
    "            # 进行拼接\n",
    "            hidden_last_out = torch.cat([hidden_last_L, hidden_last_R], dim=-1)\n",
    "            # print(hidden_last_out.shape,'hidden_last_out')   #[32, 768]\n",
    "        else:\n",
    "            hidden_last_out = hidden_last[-1]  # [32, 384]\n",
    "\n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(hidden_last_out)\n",
    "        # print(out.shape)    #[32,768]\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb12d666-421e-402c-a721-6ff6e50edb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, test_loader, optimizer):   # 训练模型\n",
    "    model.train()\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):  # 3个epoch\n",
    "        batch_idx = 0\n",
    "        for (x1, x2, x3, y) in tqdm(train_loader):\n",
    "            x1, x2, x3, y = x1.to(device), x2.to(device), x3.to(device), y.to(device)\n",
    "            y_pred = model(x1, token_type_ids=x2, attention_mask=x3)  # 得到预测结果\n",
    "            optimizer.zero_grad()             # 梯度清零\n",
    "            loss = F.cross_entropy(y_pred, y.squeeze())  # 得到loss\n",
    "            # accu_loss += loss.item() # 计算累积loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_idx += 1\n",
    "            if(batch_idx + 1) % 100 == 0:    # 打印loss\n",
    "                print('Train Epoch: {} [{}/{} ({:.2f}%)]\\tLoss: {:.6f}'.format(epoch, (batch_idx+1) * len(x1),\n",
    "                  len(train_loader.dataset),\n",
    "                  100. * batch_idx / len(train_loader),\n",
    "                  # accu_loss / batch_idx))\n",
    "                  loss.item()))  # 记得为loss.item()\n",
    "        acc = test(model, device, test_loader) # 每个epoch结束后评估一次测试集精度\n",
    "        if best_acc < acc:\n",
    "            best_acc = acc\n",
    "            torch.save(model.state_dict(), PATH)  # 保存最优模型\n",
    "def test(model, device, test_loader):    # 测试模型, 得到测试集评估结果\n",
    "#     model.eval()\n",
    "    test_loss = 0.0\n",
    "    acc = 0\n",
    "    for (x1, x2, x3, y) in tqdm(test_loader):\n",
    "        x1, x2, x3, y = x1.to(device), x2.to(device), x3.to(device), y.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_ = model(x1, token_type_ids=x2, attention_mask=x3)\n",
    "        test_loss += F.cross_entropy(y_, y.squeeze())\n",
    "        pred = y_.max(-1, keepdim=True)[1]   # .max(): 2输出，分别为最大值和最大值的index\n",
    "        acc += pred.eq(y.view_as(pred)).sum().item()    # 记得加item()\n",
    "    test_loss /= len(test_loader)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(\n",
    "          test_loss, acc, len(test_loader.dataset),\n",
    "          100. * acc / len(test_loader.dataset)))\n",
    "    return acc / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b27a3036-c21d-44b3-afc2-ea6faa1ed756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已处理1000条\n",
      "已处理2000条\n",
      "已处理3000条\n",
      "已处理4000条\n",
      "已处理5000条\n",
      "已处理6000条\n",
      "已处理7000条\n",
      "已处理8000条\n",
      "已处理9000条\n",
      "已处理10000条\n",
      "已处理11000条\n",
      "已处理12000条\n",
      "已处理13000条\n",
      "已处理14000条\n",
      "已处理15000条\n",
      "已处理16000条\n",
      "已处理17000条\n",
      "已处理18000条\n",
      "已处理19000条\n",
      "已处理20000条\n",
      "已处理21000条\n",
      "已处理22000条\n",
      "已处理23000条\n",
      "已处理24000条\n",
      "已处理25000条\n",
      "已处理26000条\n",
      "已处理27000条\n",
      "已处理28000条\n",
      "已处理29000条\n",
      "已处理30000条\n",
      "已处理31000条\n",
      "已处理32000条\n",
      "已处理33000条\n",
      "已处理34000条\n",
      "已处理35000条\n",
      "已处理36000条\n",
      "训练集处理完毕\n",
      "已处理1000条\n",
      "已处理2000条\n",
      "已处理3000条\n",
      "已处理4000条\n",
      "已处理5000条\n",
      "测试集处理完毕\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at autodl-tmp/ernie-1.0-base-zh were not used when initializing BertModel: ['ernie.encoder.layer.11.attention.output.dense.bias', 'ernie.encoder.layer.0.attention.self.query.bias', 'ernie.encoder.layer.9.attention.output.dense.weight', 'ernie.encoder.layer.8.attention.self.value.weight', 'ernie.encoder.layer.5.attention.self.key.weight', 'ernie.encoder.layer.2.attention.self.value.bias', 'ernie.encoder.layer.8.attention.self.value.bias', 'ernie.encoder.layer.8.attention.self.query.bias', 'ernie.encoder.layer.1.output.dense.weight', 'ernie.encoder.layer.2.output.dense.bias', 'ernie.encoder.layer.3.output.dense.bias', 'ernie.encoder.layer.3.attention.self.key.weight', 'ernie.encoder.layer.5.attention.self.query.weight', 'ernie.encoder.layer.7.attention.self.value.weight', 'ernie.encoder.layer.4.attention.self.key.bias', 'ernie.encoder.layer.6.attention.self.query.bias', 'ernie.encoder.layer.6.output.dense.weight', 'ernie.encoder.layer.4.attention.self.query.weight', 'ernie.encoder.layer.2.output.dense.weight', 'ernie.encoder.layer.8.attention.self.key.bias', 'ernie.encoder.layer.10.attention.output.dense.weight', 'ernie.encoder.layer.4.attention.self.value.bias', 'ernie.encoder.layer.5.intermediate.dense.weight', 'ernie.encoder.layer.9.intermediate.dense.bias', 'ernie.encoder.layer.7.attention.self.query.weight', 'ernie.encoder.layer.8.attention.output.dense.weight', 'ernie.encoder.layer.3.attention.self.value.bias', 'ernie.encoder.layer.0.attention.output.dense.weight', 'ernie.encoder.layer.11.attention.self.key.bias', 'ernie.encoder.layer.9.attention.self.value.weight', 'ernie.encoder.layer.10.attention.self.value.bias', 'ernie.encoder.layer.0.attention.self.key.bias', 'ernie.encoder.layer.3.output.dense.weight', 'ernie.encoder.layer.11.attention.output.dense.weight', 'ernie.encoder.layer.7.intermediate.dense.bias', 'ernie.encoder.layer.9.output.dense.weight', 'ernie.encoder.layer.10.output.dense.bias', 'ernie.encoder.layer.6.attention.self.key.bias', 'ernie.encoder.layer.11.output.dense.weight', 'ernie.encoder.layer.7.attention.self.value.bias', 'ernie.encoder.layer.6.attention.self.value.weight', 'ernie.encoder.layer.0.intermediate.dense.weight', 'ernie.encoder.layer.10.attention.output.dense.bias', 'ernie.encoder.layer.10.attention.self.value.weight', 'ernie.encoder.layer.10.attention.self.key.bias', 'ernie.encoder.layer.10.intermediate.dense.bias', 'ernie.encoder.layer.8.intermediate.dense.bias', 'ernie.encoder.layer.8.attention.self.query.weight', 'ernie.encoder.layer.2.attention.self.value.weight', 'ernie.encoder.layer.10.output.dense.weight', 'ernie.encoder.layer.10.intermediate.dense.weight', 'ernie.encoder.layer.0.attention.self.value.weight', 'ernie.encoder.layer.0.intermediate.dense.bias', 'ernie.encoder.layer.0.attention.self.value.bias', 'ernie.encoder.layer.3.attention.self.query.bias', 'ernie.embeddings.position_embeddings.weight', 'ernie.encoder.layer.0.output.dense.bias', 'ernie.embeddings.token_type_embeddings.weight', 'ernie.encoder.layer.4.attention.self.key.weight', 'ernie.pooler.dense.bias', 'ernie.encoder.layer.11.attention.self.key.weight', 'ernie.encoder.layer.8.intermediate.dense.weight', 'ernie.encoder.layer.7.output.dense.weight', 'ernie.embeddings.word_embeddings.weight', 'ernie.encoder.layer.0.attention.self.query.weight', 'cls.predictions.bias', 'ernie.encoder.layer.1.output.dense.bias', 'ernie.encoder.layer.1.attention.self.query.weight', 'ernie.encoder.layer.4.attention.output.dense.weight', 'ernie.encoder.layer.3.attention.self.query.weight', 'ernie.encoder.layer.6.attention.self.query.weight', 'ernie.encoder.layer.10.attention.self.key.weight', 'ernie.encoder.layer.7.attention.output.dense.weight', 'ernie.encoder.layer.7.attention.output.dense.bias', 'ernie.encoder.layer.0.output.dense.weight', 'ernie.encoder.layer.1.attention.self.query.bias', 'ernie.encoder.layer.5.attention.output.dense.bias', 'ernie.pooler.dense.weight', 'ernie.encoder.layer.10.attention.self.query.bias', 'ernie.encoder.layer.6.attention.self.key.weight', 'ernie.encoder.layer.8.output.dense.weight', 'ernie.encoder.layer.3.attention.output.dense.weight', 'ernie.encoder.layer.9.output.dense.bias', 'ernie.encoder.layer.3.intermediate.dense.weight', 'ernie.encoder.layer.9.attention.self.key.bias', 'ernie.encoder.layer.6.output.dense.bias', 'ernie.encoder.layer.5.attention.self.value.bias', 'ernie.encoder.layer.1.attention.self.value.weight', 'ernie.encoder.layer.11.attention.self.value.weight', 'ernie.encoder.layer.9.attention.self.query.bias', 'ernie.encoder.layer.1.attention.output.dense.weight', 'ernie.encoder.layer.5.attention.self.value.weight', 'ernie.encoder.layer.1.attention.output.dense.bias', 'ernie.encoder.layer.5.output.dense.weight', 'ernie.encoder.layer.3.intermediate.dense.bias', 'ernie.encoder.layer.11.attention.self.query.bias', 'ernie.encoder.layer.10.attention.self.query.weight', 'ernie.encoder.layer.9.attention.self.value.bias', 'ernie.encoder.layer.11.intermediate.dense.weight', 'ernie.encoder.layer.2.intermediate.dense.weight', 'ernie.encoder.layer.1.intermediate.dense.weight', 'ernie.encoder.layer.9.intermediate.dense.weight', 'ernie.encoder.layer.5.attention.output.dense.weight', 'ernie.encoder.layer.11.intermediate.dense.bias', 'ernie.encoder.layer.6.attention.output.dense.weight', 'ernie.encoder.layer.4.intermediate.dense.bias', 'cls.predictions.transform.dense.bias', 'ernie.encoder.layer.9.attention.self.key.weight', 'ernie.encoder.layer.11.attention.self.query.weight', 'ernie.encoder.layer.6.intermediate.dense.bias', 'ernie.encoder.layer.3.attention.self.value.weight', 'ernie.encoder.layer.4.attention.output.dense.bias', 'cls.predictions.transform.dense.weight', 'ernie.encoder.layer.9.attention.output.dense.bias', 'ernie.encoder.layer.0.attention.output.dense.bias', 'ernie.encoder.layer.1.intermediate.dense.bias', 'ernie.encoder.layer.2.attention.self.key.weight', 'ernie.encoder.layer.2.attention.self.query.weight', 'ernie.encoder.layer.1.attention.self.key.weight', 'ernie.encoder.layer.7.attention.self.key.weight', 'ernie.encoder.layer.2.attention.output.dense.bias', 'ernie.encoder.layer.4.output.dense.weight', 'ernie.encoder.layer.8.attention.output.dense.bias', 'ernie.encoder.layer.4.output.dense.bias', 'ernie.encoder.layer.11.attention.self.value.bias', 'ernie.encoder.layer.8.output.dense.bias', 'ernie.encoder.layer.7.attention.self.query.bias', 'ernie.encoder.layer.7.output.dense.bias', 'ernie.encoder.layer.2.intermediate.dense.bias', 'ernie.encoder.layer.6.attention.self.value.bias', 'ernie.encoder.layer.2.attention.self.key.bias', 'ernie.encoder.layer.3.attention.output.dense.bias', 'ernie.encoder.layer.8.attention.self.key.weight', 'ernie.encoder.layer.11.output.dense.bias', 'ernie.encoder.layer.4.attention.self.value.weight', 'ernie.encoder.layer.1.attention.self.value.bias', 'ernie.encoder.layer.2.attention.self.query.bias', 'ernie.encoder.layer.9.attention.self.query.weight', 'ernie.encoder.layer.1.attention.self.key.bias', 'ernie.encoder.layer.5.attention.self.query.bias', 'ernie.encoder.layer.3.attention.self.key.bias', 'ernie.encoder.layer.5.intermediate.dense.bias', 'ernie.encoder.layer.5.attention.self.key.bias', 'ernie.encoder.layer.5.output.dense.bias', 'ernie.encoder.layer.7.intermediate.dense.weight', 'ernie.encoder.layer.6.intermediate.dense.weight', 'ernie.encoder.layer.2.attention.output.dense.weight', 'ernie.encoder.layer.6.attention.output.dense.bias', 'ernie.encoder.layer.4.attention.self.query.bias', 'ernie.encoder.layer.7.attention.self.key.bias', 'ernie.encoder.layer.4.intermediate.dense.weight', 'ernie.encoder.layer.0.attention.self.key.weight', 'ernie.encoder.layer.1.output.LayerNorm.weight', 'ernie.encoder.layer.9.output.LayerNorm.bias', 'ernie.encoder.layer.5.attention.output.LayerNorm.bias', 'ernie.embeddings.LayerNorm.bias', 'ernie.encoder.layer.11.output.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'ernie.encoder.layer.1.attention.output.LayerNorm.weight', 'ernie.encoder.layer.10.attention.output.LayerNorm.bias', 'ernie.encoder.layer.8.output.LayerNorm.weight', 'ernie.encoder.layer.10.attention.output.LayerNorm.weight', 'ernie.encoder.layer.10.output.LayerNorm.weight', 'ernie.encoder.layer.9.attention.output.LayerNorm.bias', 'ernie.encoder.layer.2.output.LayerNorm.weight', 'ernie.encoder.layer.3.attention.output.LayerNorm.weight', 'ernie.encoder.layer.0.output.LayerNorm.weight', 'ernie.encoder.layer.4.output.LayerNorm.bias', 'ernie.encoder.layer.9.attention.output.LayerNorm.weight', 'ernie.encoder.layer.2.output.LayerNorm.bias', 'ernie.encoder.layer.8.attention.output.LayerNorm.weight', 'ernie.embeddings.LayerNorm.weight', 'ernie.encoder.layer.5.output.LayerNorm.bias', 'ernie.encoder.layer.0.output.LayerNorm.bias', 'ernie.encoder.layer.9.output.LayerNorm.weight', 'ernie.encoder.layer.8.attention.output.LayerNorm.bias', 'ernie.encoder.layer.7.attention.output.LayerNorm.weight', 'ernie.encoder.layer.3.attention.output.LayerNorm.bias', 'ernie.encoder.layer.11.attention.output.LayerNorm.bias', 'ernie.encoder.layer.6.attention.output.LayerNorm.weight', 'ernie.encoder.layer.3.output.LayerNorm.bias', 'ernie.encoder.layer.4.output.LayerNorm.weight', 'ernie.encoder.layer.7.attention.output.LayerNorm.bias', 'ernie.encoder.layer.3.output.LayerNorm.weight', 'ernie.encoder.layer.2.attention.output.LayerNorm.weight', 'ernie.encoder.layer.4.attention.output.LayerNorm.bias', 'ernie.encoder.layer.10.output.LayerNorm.bias', 'ernie.encoder.layer.5.output.LayerNorm.weight', 'ernie.encoder.layer.6.output.LayerNorm.weight', 'ernie.encoder.layer.2.attention.output.LayerNorm.bias', 'ernie.encoder.layer.6.attention.output.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.bias', 'ernie.encoder.layer.11.output.LayerNorm.weight', 'ernie.encoder.layer.1.output.LayerNorm.bias', 'ernie.encoder.layer.1.attention.output.LayerNorm.bias', 'ernie.encoder.layer.8.output.LayerNorm.bias', 'ernie.encoder.layer.11.attention.output.LayerNorm.weight', 'ernie.encoder.layer.7.output.LayerNorm.bias', 'ernie.encoder.layer.7.output.LayerNorm.weight', 'ernie.encoder.layer.5.attention.output.LayerNorm.weight', 'ernie.encoder.layer.0.attention.output.LayerNorm.weight', 'ernie.encoder.layer.6.output.LayerNorm.bias', 'ernie.encoder.layer.0.attention.output.LayerNorm.bias', 'ernie.encoder.layer.4.attention.output.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at autodl-tmp/ernie-1.0-base-zh and are newly initialized: ['embeddings.word_embeddings.weight', 'embeddings.position_embeddings.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始模型加载完毕\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 100/569 [00:13<01:06,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [6400/36374 (17.40%)]\tLoss: 1.627794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 200/569 [00:27<00:51,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [12800/36374 (34.97%)]\tLoss: 1.658862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 300/569 [00:41<00:36,  7.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [19200/36374 (52.55%)]\tLoss: 1.651330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 400/569 [00:55<00:23,  7.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [25600/36374 (70.12%)]\tLoss: 1.533785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 500/569 [01:09<00:09,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [32000/36374 (87.70%)]\tLoss: 1.274147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 569/569 [01:18<00:00,  7.22it/s]\n",
      "100%|██████████| 79/79 [00:03<00:00, 23.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.2960, Accuracy: 2592/5000 (51.84%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 100/569 [00:13<01:04,  7.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [6400/36374 (17.40%)]\tLoss: 0.917493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 200/569 [00:27<00:51,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [12800/36374 (34.97%)]\tLoss: 0.948731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 300/569 [00:41<00:37,  7.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [19200/36374 (52.55%)]\tLoss: 0.978867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 400/569 [00:55<00:23,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [25600/36374 (70.12%)]\tLoss: 0.916279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 500/569 [01:09<00:09,  7.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [32000/36374 (87.70%)]\tLoss: 0.908871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 569/569 [01:18<00:00,  7.22it/s]\n",
      "100%|██████████| 79/79 [00:03<00:00, 23.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 1.0910, Accuracy: 2999/5000 (59.98%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 100/569 [00:13<01:06,  7.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [6400/36374 (17.40%)]\tLoss: 0.945088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 200/569 [00:27<00:50,  7.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [12800/36374 (34.97%)]\tLoss: 1.123848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 300/569 [00:41<00:37,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [19200/36374 (52.55%)]\tLoss: 1.065852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 400/569 [00:55<00:23,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [25600/36374 (70.12%)]\tLoss: 0.717646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 500/569 [01:09<00:09,  7.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [32000/36374 (87.70%)]\tLoss: 0.881603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 569/569 [01:18<00:00,  7.21it/s]\n",
      "100%|██████████| 79/79 [00:03<00:00, 23.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.9776, Accuracy: 3191/5000 (63.82%)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import BatchSampler, TensorDataset, DataLoader, RandomSampler\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "SEQ_LENGTH = 128\n",
    "BATCH_SIZE = 64\n",
    "TOKENIZER = BertTokenizer.from_pretrained(MODEL_NAME) #模型所在的目录名称\n",
    "\n",
    "train_data = genDataLoader(True)\n",
    "print('训练集处理完毕')\n",
    "test_data = genDataLoader(False)\n",
    "print('测试集处理完毕')\n",
    "\n",
    "# 开始训练\n",
    "MODEL1 = BERT_BiLSTM(hidden_dim = 384, num_classes = 6, n_layers = 2, bidirectional=True, drop_prob=0.5) # 指定分类类别 hidden_dim, num_classes, n_layers, bidirectional=True, drop_prob=0.5\n",
    "print('原始模型加载完毕')\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL = MODEL1.to(DEVICE)\n",
    "OPTIMIZER = torch.optim.Adam(MODEL.parameters(), lr=2e-5) # 优化器\n",
    "NUM_EPOCHS = 3 # epoch\n",
    "\n",
    "train(MODEL, DEVICE, train_data, test_data, OPTIMIZER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14eba2e6-dfeb-4375-86ee-4598e818d7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72543, 14)\n",
      "(72543, 14)\n"
     ]
    }
   ],
   "source": [
    "#开始预测\n",
    "data_path='autodl-tmp/pred_data/wuhannomeaning.csv'\n",
    "res_path='autodl-tmp/res_data_315/wuhannomeaning.csv'\n",
    "\n",
    "# data_path='autodl-tmp/pred_data/shanghai.csv'\n",
    "# res_path='autodl-tmp/res_data_315/shanghai.csv'\n",
    "\n",
    "targrt='content'\n",
    "data = pd.read_csv(data_path)\n",
    "LABEL_DICT_new = dict(zip(LABEL_DICT.values(), LABEL_DICT.keys()))\n",
    "print(data.shape)\n",
    "data.dropna(subset=[targrt], inplace=True)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b145d7fe-9501-47ca-b5fa-946efa9348ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已处理1000条\n",
      "已处理2000条\n",
      "已处理3000条\n",
      "已处理4000条\n",
      "已处理5000条\n",
      "已处理6000条\n",
      "已处理7000条\n",
      "已处理8000条\n",
      "已处理9000条\n",
      "已处理10000条\n",
      "已处理11000条\n",
      "已处理12000条\n",
      "已处理13000条\n",
      "已处理14000条\n",
      "已处理15000条\n",
      "已处理16000条\n",
      "已处理17000条\n",
      "已处理18000条\n",
      "已处理19000条\n",
      "已处理20000条\n",
      "已处理21000条\n",
      "已处理22000条\n",
      "已处理23000条\n",
      "已处理24000条\n",
      "已处理25000条\n",
      "已处理26000条\n",
      "已处理27000条\n",
      "已处理28000条\n",
      "已处理29000条\n",
      "已处理30000条\n",
      "已处理31000条\n",
      "已处理32000条\n",
      "已处理33000条\n",
      "已处理34000条\n",
      "已处理35000条\n",
      "已处理36000条\n",
      "已处理37000条\n",
      "已处理38000条\n",
      "已处理39000条\n",
      "已处理40000条\n",
      "已处理41000条\n",
      "已处理42000条\n",
      "已处理43000条\n",
      "已处理44000条\n",
      "已处理45000条\n",
      "已处理46000条\n",
      "已处理47000条\n",
      "已处理48000条\n",
      "已处理49000条\n",
      "已处理50000条\n",
      "已处理51000条\n",
      "已处理52000条\n",
      "已处理53000条\n",
      "已处理54000条\n",
      "已处理55000条\n",
      "已处理56000条\n",
      "已处理57000条\n",
      "已处理58000条\n",
      "已处理59000条\n",
      "已处理60000条\n",
      "已处理61000条\n",
      "已处理62000条\n",
      "已处理63000条\n",
      "已处理64000条\n",
      "已处理65000条\n",
      "已处理66000条\n",
      "已处理67000条\n",
      "已处理68000条\n",
      "已处理69000条\n",
      "已处理70000条\n",
      "已处理71000条\n",
      "已处理72000条\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1134/1134 [00:47<00:00, 23.93it/s]\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "correct = 0\n",
    "wrong = 0\n",
    "count = 0\n",
    "ids_pool = []\n",
    "types_pool = []\n",
    "masks_pool = []\n",
    "target_pool = []\n",
    "count = 0\n",
    "\n",
    "import torch.nn.functional as F\n",
    "names=list(LABEL_DICT_new.values())\n",
    "for id_item,each in data.iterrows():\n",
    "    cur_sentence = each[targrt]\n",
    "    ids = []\n",
    "    types = []\n",
    "    masks = []\n",
    "    cur_ids, cur_type, cur_mask = convert_text_to_token(TOKENIZER, each[targrt], seq_length=SEQ_LENGTH)\n",
    "    # ids.append(cur_ids)\n",
    "    # types.append(cur_type)\n",
    "    # masks.append(cur_mask)\n",
    "    # cur_ids, cur_type, cur_mask = torch.LongTensor(np.array(ids)).to(DEVICE), torch.LongTensor(np.array(types)).to(DEVICE), torch.LongTensor(np.array(masks)).to(DEVICE) # 数据构造成tensor形式\n",
    "    ids_pool.append(cur_ids)\n",
    "    types_pool.append(cur_type)\n",
    "    masks_pool.append(cur_mask)\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print('已处理{}条'.format(count))\n",
    "data_gen = TensorDataset(torch.LongTensor(np.array(ids_pool)),\n",
    "                  torch.LongTensor(np.array(types_pool)),\n",
    "                  torch.LongTensor(np.array(masks_pool)))\n",
    "loader = DataLoader(data_gen, batch_size=BATCH_SIZE,shuffle=False)\n",
    "res_df=pd.DataFrame([])\n",
    "MODEL.eval()\n",
    "names=list(LABEL_DICT_new.values())\n",
    "\n",
    "col=names+['flag']\n",
    "res_df=pd.DataFrame(columns=col)\n",
    "for (x1, x2, x3) in tqdm(loader):\n",
    "    x1, x2, x3 = x1.to(device), x2.to(device), x3.to(device)\n",
    "    with torch.no_grad():\n",
    "        y_ = MODEL(x1, token_type_ids=x2, attention_mask=x3)\n",
    "        probabilities = F.softmax(y_, dim=-1)\n",
    "        pred = probabilities.max(-1, keepdim=True)[1]  # 取最大值\n",
    "        probabilities=probabilities.cuda().data.cpu().numpy()\n",
    "        pred=pred.cuda().data.cpu().numpy()\n",
    "        labs=[names[x[0]] for x in pred]\n",
    "        \n",
    "        # 将列表转换为 Pandas DataFrame\n",
    "        \n",
    "        df_A = pd.DataFrame(probabilities)\n",
    "        df_B = pd.DataFrame(labs)\n",
    "\n",
    "        # 将 B 作为最后一列合并到 A\n",
    "        result = pd.concat([df_A, df_B], axis=1)\n",
    "        result.columns =col\n",
    "        \n",
    "        res_df=pd.concat([res_df,result], axis=0, ignore_index=True)\n",
    "res_data=pd.concat([data,res_df], axis=1)\n",
    "res_data.to_csv(res_path,encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4757d3e-7e71-4a68-a953-aae672b52292",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
